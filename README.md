# ğŸ§  Neural Network Implementation & Evaluation  

## ğŸ“œ Overview  
This project involves modifying the **NNDL book's "network.py"** code (Chapter 1) to improve its functionality and performance. The goal is to deepen the understanding of neural networks by implementing **evaluation metrics, efficiency improvements, and visualization techniques**.  

## ğŸ¯ Objectives  
- Modify the **network definition code** to include additional evaluation metrics.  
- Optimize **SGD, backpropagation, and mini-batch updates** for efficiency.  
- Implement **early stopping** based on accuracy and MSE loss trends.  
- Develop a **custom test application** to validate the modified network.  
- Visualize **training performance metrics** (Accuracy, MSE, CE, and Log-Likelihood).  

ğŸ“Œ **Dataset**: The **Iris dataset** (iris-3.csv) is used for training and testing.  

## âš™ï¸ Installation & Dependencies  
### **Requirements**  
- **Python 3.x**  
- **Jupyter Notebook**  
- **Required Libraries**:  
  - `numpy` â€“ Matrix computations  
  - `matplotlib` â€“ Data visualization  
  - `pandas` â€“ Dataset handling  
  - `scipy` â€“ Scientific computations  

## ğŸ” Project Structure
### 1ï¸âƒ£ Initial Setup & Testing
ğŸ“Œ **Objective**: Ensure environment compatibility and run baseline tests.

### 2ï¸âƒ£ Modifications to the Network Code
ğŸ“Œ **Objective**: Extend the existing evaluate(), SGD(), backprop(), and update_mini_batch() functions.

âœ… **Key Modifications**:
- **evaluate() function**: Compute and return
  - âœ… Correctly classified instances (Count)
  - âœ… Accuracy (Accuracy)
  - âœ… Mean Squared Error (MSE)
  - âœ… Cross-Entropy (CE)
  - âœ… Log-Likelihood (LL)
- **SGD() function**:
  - Include **test data evaluation** in a similar format.
  - Implement **early stopping** if accuracy reaches **1.0** or **MSE stops decreasing** (checks last three epochs for stagnation).
  - Return **training and test performance metrics** as nested lists [train_results, test_results].
- **backprop() function**:
  - Preallocate activation matrices for **all layers** to improve efficiency.
- **update_mini_batch() function**:
  - Optimize weight updates by accumulating gradients **without redundant memory allocations.**

### 3ï¸âƒ£ Testing & Validation
ğŸ“Œ **Objective**: Verify the correctness of the modified network.

âœ… **Steps**:
- Run the second test application.
- Ensure it works with a **deeper network** (iris4-20-7-3.dat).

### 4ï¸âƒ£ Custom Test Application & Visualization
ğŸ“Œ **Objective**: Implement and visualize a new test case.
- Given graphs here are using eta = 0.315, so eta 0.3 should yields consistent results on the performances of both train and test sets. From the graphs, Accuracy increases over time, at epoch 100, trainset yields 98.10% and test set yields 93.33%. Mean-Squared Error, Cross Entropy and Log Likelihood are decreasing over time and both train and test sets.
- Around epoch 60th forward, errors on testing data are slightly higher than training data but not significant. The two lines are almost overlap indicates there is no underfitting/overfitting of the data using eta approximately 0.3. The neural network is performing well on unseen data, correctly classifying the three Iris species.
![Neural Network](https://github.com/pngo1997/Images/blob/main/Neural%20Network.png)  

## ğŸ“Š Results & Findings
âœ… Successfully extended the evaluation metrics (Accuracy, MSE, CE, LL).

âœ… Optimized backpropagation and mini-batch processing for better efficiency.

âœ… Implemented early stopping based on accuracy and loss trends.

âœ… Verified correctness using multiple test cases.

âœ… Created visualizations for training progress.
